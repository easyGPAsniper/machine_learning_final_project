{"metadata":{"accelerator":"TPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"premium","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport os \nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import ToTensor\nimport cv2\nimport pickle as pkl","metadata":{"id":"FqkJUasjk9gB","execution":{"iopub.status.busy":"2022-12-15T01:23:07.436950Z","iopub.execute_input":"2022-12-15T01:23:07.437705Z","iopub.status.idle":"2022-12-15T01:23:07.443708Z","shell.execute_reply.started":"2022-12-15T01:23:07.437667Z","shell.execute_reply":"2022-12-15T01:23:07.442675Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"3hXwMn4rlQ0b","execution":{"iopub.status.busy":"2022-12-15T01:23:07.449238Z","iopub.execute_input":"2022-12-15T01:23:07.449924Z","iopub.status.idle":"2022-12-15T01:23:07.455860Z","shell.execute_reply.started":"2022-12-15T01:23:07.449884Z","shell.execute_reply":"2022-12-15T01:23:07.454961Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"class LazyLoadDataset(Dataset):\n  def __init__(self, path, train=True, transform=None):\n    self.transform = transform\n    path = path + (\"train/\" if train else \"test/\")\n    self.train = train\n    if(train):\n        self.pathX = path + \"X/\"\n        self.pathY = path + \"Y/\"\n    else:\n        self.pathX = path + \"X/\"\n    self.data = os.listdir(self.pathX)\n\n  def __getitem__(self, idx):\n    f = self.data[idx]\n\n    # X\n    # read rgb images\n    img0 = cv2.imread(self.pathX + f + \"/rgb/0.png\")\n    img1 = cv2.imread(self.pathX + f + \"/rgb/1.png\")\n    img2 = cv2.imread(self.pathX + f + \"/rgb/2.png\")\n    if self.transform is not None:\n      img0 = self.transform(img0)\n      img1 = self.transform(img1)\n      img2 = self.transform(img2)\n\n    # read depth images \n    depth = np.load(self.pathX + f + \"/depth.npy\")\n    # read field ID\n    field_id = pkl.load(open(self.pathX + f + \"/field_id.pkl\", \"rb\"))\n\n    # Y\n    if(self.train):\n        Y = np.load(self.pathY + f + \".npy\")\n        return (img0, img1, img2, depth/1000, field_id), Y*1000\n    else:\n        return (img0, img1, img2, depth/1000, field_id)\n  def __len__(self):\n    return len(self.data)","metadata":{"id":"UtR5a6P9Do5E","execution":{"iopub.status.busy":"2022-12-15T01:23:07.832283Z","iopub.execute_input":"2022-12-15T01:23:07.832642Z","iopub.status.idle":"2022-12-15T01:23:07.845266Z","shell.execute_reply.started":"2022-12-15T01:23:07.832605Z","shell.execute_reply":"2022-12-15T01:23:07.844086Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def get_min_and_max_depth(data):\n    min_depth, max_depth = 10000,0\n    for image, _ in data:\n        img0, img1, img2, depth, field_id = image\n        single_min = np.min(depth)\n        single_max = np.max(depth)\n        if(single_min<min_depth):\n            min_depth = single_min\n        if(single_max>max_depth):\n            max_depth = single_max\n    return min_depth, max_depth\n\ndef get_mean_and_std(data):\n    means = torch.tensor([0.0, 0.0, 0.0])\n    stds = torch.tensor([0.0, 0.0, 0.0])\n    for image, _ in data:\n        img0, img1, img2, depth, field_id = image\n        img0_reshaped = img0.reshape(3, -1)\n        img1_reshaped = img1.reshape(3, -1)\n        img2_reshaped = img2.reshape(3, -1)\n        image_cat = torch.cat((img0_reshaped, img1_reshaped, img2_reshaped), dim=1)\n        img_mean = image_cat.mean(axis=1)\n        img_std = image_cat.std(axis=1)  \n        means += img_mean\n        stds += img_std \n    means = means/len(data.data)\n    stds = stds/len(data.data)\n    return means,stds","metadata":{"execution":{"iopub.status.busy":"2022-12-15T01:23:07.873079Z","iopub.execute_input":"2022-12-15T01:23:07.873332Z","iopub.status.idle":"2022-12-15T01:23:07.884146Z","shell.execute_reply.started":"2022-12-15T01:23:07.873309Z","shell.execute_reply":"2022-12-15T01:23:07.882895Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nalldata = LazyLoadDataset(\"/kaggle/input/lazydata/lazydata/\", train = True, transform = transforms.Compose([transforms.ToTensor()]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"means,stds = get_mean_and_std(alldata)\nmin_depth, max_depth = get_min_and_max_depth(alldata)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T01:23:07.885952Z","iopub.execute_input":"2022-12-15T01:23:07.886810Z","iopub.status.idle":"2022-12-15T01:25:32.184062Z","shell.execute_reply.started":"2022-12-15T01:23:07.886718Z","shell.execute_reply":"2022-12-15T01:25:32.182958Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"print(means,stds)\nprint(min_depth, max_depth)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T01:25:32.187656Z","iopub.execute_input":"2022-12-15T01:25:32.188418Z","iopub.status.idle":"2022-12-15T01:25:32.199164Z","shell.execute_reply.started":"2022-12-15T01:25:32.188377Z","shell.execute_reply":"2022-12-15T01:25:32.197743Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"tensor([0.4851, 0.4623, 0.4356]) tensor([0.2226, 0.2206, 0.2359])\n0.0 65.535\n","output_type":"stream"}]},{"cell_type":"code","source":"transforms_train = transforms.Compose([\n#     transforms.RandomInvert(),\n#     transforms.RandomAdjustSharpness(sharpness_factor=4),\n#     transforms.RandomAutocontrast(),\n#     transforms.RandomSolarize(threshold=192.0),\n#     transforms.RandomPosterize(bits=2),\n#     transforms.ColorJitter(brightness=1.0,contrast=1.0, hue=0.5), # data augmentation\n    transforms.ToTensor(),\n    transforms.Normalize(mean = means, std = stds) # normalization\n])","metadata":{"execution":{"iopub.status.busy":"2022-12-15T01:25:32.201529Z","iopub.execute_input":"2022-12-15T01:25:32.202749Z","iopub.status.idle":"2022-12-15T01:25:32.208561Z","shell.execute_reply.started":"2022-12-15T01:25:32.202712Z","shell.execute_reply":"2022-12-15T01:25:32.207365Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"train_dataset = LazyLoadDataset(\"/kaggle/input/lazydata/lazydata/\", True, transforms_train)\n# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","metadata":{"id":"j-1946pQFmJX","execution":{"iopub.status.busy":"2022-12-15T01:25:32.210425Z","iopub.execute_input":"2022-12-15T01:25:32.211400Z","iopub.status.idle":"2022-12-15T01:25:32.220432Z","shell.execute_reply.started":"2022-12-15T01:25:32.211362Z","shell.execute_reply":"2022-12-15T01:25:32.219482Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ntry:\n    from torch.hub import load_state_dict_from_url\nexcept ImportError:\n    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(BasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n        if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.downsample is not None:\n            identity = self.downsample(x)\n        out += identity\n        out = self.relu(out)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n    # This variant is also known as ResNet V1.5 and improves accuracy according to\n    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n    expansion = 4\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv3(out)\n        out = self.bn3(out)\n        if self.downsample is not None:\n            identity = self.downsample(x)\n        out += identity\n        out = self.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=12, zero_init_residual=False,\n                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n                 norm_layer=None):\n        super(ResNet, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            # each element in the tuple indicates if we should replace\n            # the 2x2 stride with a dilated convolution instead\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\"replace_stride_with_dilation should be None \"\n                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(12, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n                                       dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                       dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                       dilate=replace_stride_with_dilation[2])\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n                            self.base_width, previous_dilation, norm_layer))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=self.groups,\n                                base_width=self.base_width, dilation=self.dilation,\n                                norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def _forward_impl(self, x):\n        # See note [TorchScript super()]\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\n    def forward(self, x):\n        return self._forward_impl(x)\n    \n    def freeze_parameters(self, freeze_layer4=False, train_fc=False):\n        for p in self.conv1.parameters():\n            p.requires_grad = False\n        for p in self.bn1.parameters():\n            p.requires_grad = False\n        for p in self.layer1.parameters():\n            p.requires_grad = False\n        for p in self.layer2.parameters():\n            p.requires_grad = False\n        # for p in self.layer3.parameters():\n        #         p.requires_grad = False\n        for p in self.fc.parameters():\n            p.requires_grad = False\n\n        if freeze_layer4:\n            for p in self.layer4.parameters():\n                p.requires_grad = False\n        if train_fc:\n            for p in self.fc.parameters():\n                p.requires_grad = True\n\n        counter = 0\n        for p in self.parameters():\n            if p.requires_grad:\n                counter += 1\n        print('n of req grad params: {}, n of total parameters: {}'.format(counter, len(list(self.parameters()))))\n\ndef _resnet(block, layers, pretrained, progress, **kwargs):\n    model = ResNet(block, layers, **kwargs)\n    return model\n\ndef resnet18(pretrained=False, progress=True, **kwargs):\n    return _resnet(BasicBlock, [2, 2, 2, 2], pretrained, progress,\n                   **kwargs)\n\ndef resnet34(pretrained=False, progress=True, **kwargs):\n    return _resnet(BasicBlock, [3, 4, 6, 3], pretrained, progress,\n                   **kwargs)\n\ndef resnet50(pretrained=False, progress=True, **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs, num_classes=365)\n    return model\n\ndef resnet101(pretrained=False, progress=True, **kwargs):\n    return _resnet(Bottleneck, [3, 4, 23, 3], pretrained, progress,\n                   **kwargs)\n\ndef resnet152(pretrained=False, progress=True, **kwargs):\n    return _resnet(Bottleneck, [3, 8, 36, 3], pretrained, progress,\n                   **kwargs)\n\n","metadata":{"id":"uZrya0aH5C8I","execution":{"iopub.status.busy":"2022-12-15T01:25:32.222171Z","iopub.execute_input":"2022-12-15T01:25:32.222763Z","iopub.status.idle":"2022-12-15T01:25:32.266735Z","shell.execute_reply.started":"2022-12-15T01:25:32.222727Z","shell.execute_reply":"2022-12-15T01:25:32.265595Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"class RMSELoss(nn.Module):\n    def __init__(self, eps=1e-6):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        self.eps = eps\n        \n    def forward(self,yhat,y):\n        loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n        return loss","metadata":{"execution":{"iopub.status.busy":"2022-12-15T01:25:32.268896Z","iopub.execute_input":"2022-12-15T01:25:32.269926Z","iopub.status.idle":"2022-12-15T01:25:32.283393Z","shell.execute_reply.started":"2022-12-15T01:25:32.269874Z","shell.execute_reply":"2022-12-15T01:25:32.282374Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# model = resnet101().to(device)\n# model = resnet50().to(device)\nmodel = resnet152().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n#                                               step_size=10,\n#                                               gamma=0.1,)","metadata":{"id":"_hfDNp-ZL3eq","execution":{"iopub.status.busy":"2022-12-15T01:25:32.284796Z","iopub.execute_input":"2022-12-15T01:25:32.285644Z","iopub.status.idle":"2022-12-15T01:25:33.341191Z","shell.execute_reply.started":"2022-12-15T01:25:32.285608Z","shell.execute_reply":"2022-12-15T01:25:33.340117Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def train(dataloader, model, optimizer):\n    size = len(dataloader.dataset)\n    model.train()\n\n    for batch_index, (data, target) in enumerate(dataloader):\n        img0, img1, img2, depth, field_id = data\n        depth = np.array(depth)\n        depth = torch.from_numpy(depth)\n        depth = (depth - min_depth) / (max_depth - min_depth)\n        RGBD = torch.cat((img0, img1, img2, depth), dim=1)\n        RGBD = RGBD.type(torch.float).to(device)\n        target = target.type(torch.float).to(device)\n        pred = model(RGBD)\n        loss_fn = RMSELoss()\n        loss = loss_fn(pred, target)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch_index % 20 == 0:\n            loss, batch = loss.item(), batch_index * len(RGBD)\n            print(f\"loss: {loss:>3f} now processing:{batch} /3396 \")\n            ","metadata":{"id":"IlGacHvFO9AL","execution":{"iopub.status.busy":"2022-12-15T02:59:53.470885Z","iopub.execute_input":"2022-12-15T02:59:53.471797Z","iopub.status.idle":"2022-12-15T02:59:53.484486Z","shell.execute_reply.started":"2022-12-15T02:59:53.471732Z","shell.execute_reply":"2022-12-15T02:59:53.483178Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"for epoch in range(10):\n    print(epoch)\n    train(train_dataloader, model, optimizer)","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"aMF6PIxBznAV","outputId":"b5aa8edf-3aab-4559-ce33-f7b43da5312c","execution":{"iopub.status.busy":"2022-12-15T02:59:55.066712Z","iopub.execute_input":"2022-12-15T02:59:55.067801Z","iopub.status.idle":"2022-12-15T02:59:56.329777Z","shell.execute_reply.started":"2022-12-15T02:59:55.067753Z","shell.execute_reply":"2022-12-15T02:59:56.327728Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/2419445584.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_23/2467736941.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, optimizer)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 14.76 GiB total capacity; 13.15 GiB already allocated; 3.75 MiB free; 13.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 14.76 GiB total capacity; 13.15 GiB already allocated; 3.75 MiB free; 13.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"test_dataset = LazyLoadDataset(\"/kaggle/input/lazydata/lazydata/\", train = False, transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean = means, std = stds )]))\ntest_dataloader = DataLoader(test_dataset, batch_size = 3)","metadata":{"id":"xDIqceQzT3qT","execution":{"iopub.status.busy":"2022-12-15T01:53:18.775725Z","iopub.execute_input":"2022-12-15T01:53:18.776536Z","iopub.status.idle":"2022-12-15T01:53:18.787963Z","shell.execute_reply.started":"2022-12-15T01:53:18.776496Z","shell.execute_reply":"2022-12-15T01:53:18.787100Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"import pickle\nimport pandas as pd\n\noutfile = 'submission.csv'\n\noutput_file = open(outfile, 'w')\n\ntitles = ['ID', 'FINGER_POS_1', 'FINGER_POS_2', 'FINGER_POS_3', 'FINGER_POS_4', 'FINGER_POS_5', 'FINGER_POS_6',\n         'FINGER_POS_7', 'FINGER_POS_8', 'FINGER_POS_9', 'FINGER_POS_10', 'FINGER_POS_11', 'FINGER_POS_12']\npreds = []\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nindices = []\nmodel.eval()\n\nfor batch, data in enumerate(test_dataloader):\n        img0, img1, img2, depth, index = data\n        depth = np.array(depth)\n        depth = torch.from_numpy(depth)\n        depth = (depth - min_depth) / (max_depth - min_depth)\n        data = torch.cat((img0, img1, img2, depth), dim=1).type(torch.float)\n        output = model(data.to('cuda')) \n        for i in range(output.shape[0]):\n            preds.append(output[i].cpu().detach().numpy())\n            indices.append(index[i])\n\ndf = pd.concat([pd.DataFrame(indices), pd.DataFrame.from_records(preds)/1000], axis = 1, names = titles)\ndf.columns = titles\ndf.to_csv(outfile, index = False)\nprint(\"Written to csv file {}\".format(outfile))","metadata":{"id":"cveqFLnXEhfE","execution":{"iopub.status.busy":"2022-12-15T02:53:51.163170Z","iopub.execute_input":"2022-12-15T02:53:51.163551Z","iopub.status.idle":"2022-12-15T02:54:18.600304Z","shell.execute_reply.started":"2022-12-15T02:53:51.163514Z","shell.execute_reply":"2022-12-15T02:54:18.599098Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Written to csv file submission.csv\n","output_type":"stream"}]}]}